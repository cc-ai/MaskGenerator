output_path: ./outputs/

# ----------------
# ----- Data -----
# ----------------
data:
  files: # if one is not none it will override the dirs location
    base: /path/to/data
    train: train.json
    val: val.json

  loaders:
    batch_size: 20
    shuffle: True
    num_workers: 3
  transforms:
    - name: hflip
      ignore: false
      p: 0.5
    - name: resize
      ignore: false
      new_size: 256
    - name: crop
      ignore: false
      height: 224
      width: 224
    - name: resize # ? this or change generator's output? Or resize larger then crop to 256?
      ignore: false
      new_size: 256

# ---------------------
# ----- Generator -----
# ---------------------
gen:
  optim:
    optimizer: ExtraAdam # one in [Adam, ExtraAdam] default: Adam
    beta1: 0.9
    lr: 0.0005
    lr_policy: step # constant or step ; if step, specify step_size and gamma
    lr_step_size: 30 # for linear decay : period of learning rate decay (epochs)
    lr_gamma: 0.5 # Multiplicative factor of learning rate decay
  default:
    &default-gen # default parameters for the generator (encoder and decoders)
    activ: lrelu # activation function [relu/lrelu/prelu/selu/tanh]
    init_gain: 0.2
    init_type: kaiming
    n_res: 4 # number of residual blocks before upsampling
    n_upsample: 2 # upsampling in spade decoder ; should match encoder.n_downsample
    pad_type: reflect # padding type [zero/reflect]
    res_dim: 256 # Resblock number of channels (=latent space's)
    res_norm: "instance" # normalization to apply

  encoder: # specific params for the encoder
    <<: *default-gen
    dim: 4 # dimension of the first projection before downsamplings
    input_dim: 3 # input number of channels
    n_downsample: 4 # number of downsampling layers in encoder
    n_res: 4 # number of residual blocks in content encoder/decoder
    norm: "instance"
  decoder:
    <<: *default-gen
    output_dim: 1
    output_activ: "sigmoid"
    norm: "instance"

# ---------------------
# ----- Generator -----
# ---------------------

dis:
  soft_shift: 0.2 # label smoothing: real in U(1-soft_shift, 1), fake in U(0, soft_shift) # ! one-sided label smoothing
  flip_prob: 0.05 # label flipping
  optim:
    optimizer: ExtraAdam # one in [Adam, ExtraAdam] default: Adam
    beta1: 0.5
    lr: 0.0005
    lr_policy: step # constant or step ; if step, specify step_size and gamma
    lr_step_size: 30 # for linear decay
    lr_gamma: 0.5
  default:
    #&default-dis # default setting for discriminators (there are 4 of them for rn rf sn sf)
    input_nc: 3
    ndf: 64
    n_layers: 3
    norm: "instance"
    init_type: kaiming
    init_gain: 0.2
    use_sigmoid: false
    kw: 4
    padw: 1
    nf_mult: 1
    nf_mult_prev: 1

# ------------------------
# ----- Model options -----
# ------------------------

model:
  model_name: ""
  is_train: true
  use_gpu: true
  lambdas: {}
  loss_names: []
  verbose: True

# ------------------------
# ----- Comet options ----
# ------------------------

comet:
  workspace: ""
  project_name: ""
  exp: None
  log_image_freq: 1000

# ------------------------
# ----- Train Params -----
# ------------------------
train:
  epochs: 1000
  lambdas: # scaling factors in the total loss
  log_level: 1 # 0: no log, 1: only aggregated losses, >1 detailed losses
  save_im_freq: 100
  save_im: False

# -----------------------------
# ----- Validation Params -----
# -----------------------------
val:
  max_log_images: 1
  save_im_freq: 1000
  store_images: false # write to disk on top of comet logging
  overlay: false
